# Links

- [How QLoRA, a Revolutionary Fine-Tuning Method, Works](https://brev.dev/blog/how-qlora-works)
- https://github.com/ml-explore/mlx-examples
- https://medium.com/@ingridwickstevens/comfyui-stable-diffusion-installation-for-apple-silicon-m1-m2-m3-a7bd78e86495
- https://civitai.com/
- https://mer.vin/2024/02/mlx-mistral-lora-fine-tuning/
- https://docs.github.com/en/repositories/working-with-files/managing-large-files/installing-git-large-file-storage
- https://github.com/apple/ml-stable-diffusion?tab=readme-ov-file
- https://github.com/apple/ml-stable-diffusion
- https://github.com/CompVis/stable-diffusion
- https://www.reddit.com/r/LocalLLaMA/comments/196kc6l/my_first_finetune_mistral7bv01greeceromev01_for/?rdt=58860
- https://www.reddit.com/r/MachineLearning/comments/17ky0g6/macbook_pro_m3_for_llms_and_pytorch_d/
- https://apeatling.com/articles/part-3-fine-tuning-your-llm-using-the-mlx-framework/
- https://github.com/eliben/code-for-blog/tree/master/2024/ollama-go-clients
- https://eli.thegreenplace.net/2023/retrieval-augmented-generation-in-go/
- https://github.com/apeatling/simple-guide-to-mlx-finetuning/tree/trunk
- https://medium.com/@ingridwickstevens/mlx-stable-diffusion-for-local-image-generation-on-apple-silicon-2ec00ba1031a
- https://levelup.gitconnected.com/an-ultimate-guide-to-run-any-llm-locally-eb1a43052053
- https://towardsdatascience.com/how-to-generate-instruction-datasets-from-any-documents-for-llm-fine-tuning-abb319a05d91
- https://github.com/linuxandchill/crewai-stocks-yt.git
- https://github.com/ggerganov/llama.cpp/
- https://www.youtube.com/watch?v=2Acht_5_HTo&ab_channel=AlexZiskind
- https://github.com/unslothai/unsloth
- https://huggingface.co/mistralai/Mixtral-8x7B-v0.1/discussions/10#6577e443e390cfd40990deff
- https://github.com/hiyouga/LLaMA-Factory.git
- https://github.com/OpenAccess-AI-Collective/axolotl
- https://huggingface.co/docs/trl/sft_trainer
- https://www.youtube.com/watch?v=s-jYxgKMqRc&ab_channel=SamWitteveen
- https://docs.crewai.com/how-to/LLM-Connections/#setting-up-ollama
- https://github.com/arcee-ai/mergekit/blob/main/docs/moe.md
- https://towardsdatascience.com/merge-large-language-models-with-mergekit-2118fb392b54
- https://slgero.medium.com/merge-large-language-models-29897aeb1d1a
- https://huggingface.co/spaces/mlabonne/model-family-tree
- https://arxiv.org/pdf/2401.02994.pdf
- https://github.com/18907305772/FuseLLM
- https://arxiv.org/pdf/2401.10491.pdf
- https://www.youtube.com/watch?v=eKDz-K3UvbY&ab_channel=AIAnytime
- https://huggingface.co/epfl-llm/meditron-70b
- https://blog.gopenai.com/lets-build-a-completely-open-source-rag-system-for-question-answering-on-azure-ml-documentation-c136831ba117
- https://github.com/OpenAccess-AI-Collective/axolotl/blob/main/examples/colab-notebooks/colab-axolotl-example.ipynb
- https://fka.gumroad.com/l/art-of-chatgpt-prompting 
- https://github.com/ollama/ollama/blob/main/docs/import.md
- https://chat.openai.com/share/eef389ac-7181-4eef-bd91-fa4b45661da9
- https://aviralrma.medium.com/understanding-llm-parameters-c2db4b07f0ee
- https://towardsdatascience.com/fine-tune-a-mistral-7b-model-with-direct-preference-optimization-708042745aac
- https://github.com/nalaso/devika/tree/main?tab=readme-ov-file#getting-started
- https://blog.gopenai.com/how-to-fine-tune-llama-2-on-mac-studio-4b42f317c975
- https://towardsdatascience.com/mlx-vs-mps-vs-cuda-a-benchmark-c5737ca6efc9
- https://github.com/eliben/code-for-blog/tree/master/2023/ollama-go-langchain
- https://github.com/eliben/code-for-blog/tree/master/2023/go-rag-openai
- https://towardsdatascience.com/apple-m2-max-gpu-vs-nvidia-v100-p100-and-t4-8b0d18d08894
- https://github.com/langchain-ai/rag-from-scratch/blob/main/rag_from_scratch_1_to_4.ipynb
- https://github.com/gkamradt/LLMTest_NeedleInAHaystack
- https://www.youtube.com/watch?v=KXG2f-So9oo&ab_channel=AIExplained
- https://github.com/langchain-ai/langgraph/blob/main/examples/rag/langgraph_adaptive_rag_cohere.ipynb
- https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0
- https://www.youtube.com/watch?v=r3DC_gjFCSA&ab_channel=Weights%26Biases
- https://github.com/RexiaAI/codeExamples/tree/main
- https://github.com/sugarforever/chat-ollama
- https://medium.com/@odhitom09/running-openais-server-locally-with-llama-cpp-5f29e0d955b7
- https://github.com/danielmiessler/fabric
- https://medium.com/the-ai-forum/create-a-blog-writer-multi-agent-system-using-crewai-and-ollama-f47654a5e1cd
- [Beyond the Hype: A Realistic Look at Large Language Models • Jodie Burchell • GOTO 2024](https://www.youtube.com/watch?v=Pv0cfsastFs)
- https://github.com/arcee-ai/aws-samples
- https://www.databricks.com/blog/llm-inference-performance-engineering-best-practices
- https://www.toolify.ai/gpts/build-a-powerful-llm-app-with-langchain-golang-111552
- https://www.deeplearning.ai/short-courses/langchain-for-llm-application-development/
- https://github.com/eliben/code-for-blog/tree/master/2023/ollama-go-langchain
- https://tmc.github.io/langchaingo/docs/
- https://www.linkedin.com/posts/devevantelista_rag-with-golang-langchain-and-qdrant-activity-7171356252210380800-TjWQ