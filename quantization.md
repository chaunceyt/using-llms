# Quantization

floating point ---> int 2,4,6,8 bit

- reduce the model size
- speed up computations (infrence speed)
- lower energy consumption

Types of quantization techniques

- PTQ post training quantization (llama.cpp)
- QAT quantization aware training
- Dynamic quantization

